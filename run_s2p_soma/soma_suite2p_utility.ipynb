{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca20bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing soma_suite2p_utility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile soma_suite2p_utility.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from soma_detector_utility import *\n",
    "    #this is where all the detector functions will be used; at least initially\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SUITE2P_STRUCTURE describes the sequence of directories to traverse to arrive at the files named in the key.\n",
    "For example: \"F\": [\"suite2p\", \"plane0\", \"F.npy\"] --> File describing F is found at ./suite2p/plane0/F.npy.\n",
    "All locations are relative to the suite2p output folder, which is output into the folder / file location where the original \n",
    ".tiff image for analysis was located. suite2p output can also be identified by checking if it follows this structure.\n",
    "Both spks and iscell are not used because: spks are not calculated; no need for deconvolution\n",
    "iscell is not used because all ROIs are considered and filtered later based on individual stats\n",
    "\"\"\"\n",
    "SUITE2P_STRUCTURE = {\n",
    "    \"F\": [\"suite2p\", \"plane0\", \"F.npy\"],\n",
    "    \"Fneu\": [\"suite2p\", \"plane0\", \"Fneu.npy\"],\n",
    "    #'spks': [\"suite2p\", \"plane0\", \"spks.npy\"],\n",
    "    \"stat\": [\"suite2p\", \"plane0\", \"stat.npy\"],\n",
    "    \"iscell\": [\"suite2p\", \"plane0\", \"iscell.npy\"],\n",
    "}\n",
    "\"\"\" spks is not really necessary with our current set up since the spont. events are all pretty uniform, and are below \n",
    "    AP threshold (and therefore will not need to be deconvolved into action potentials themselves)\"\"\"\n",
    "\n",
    "def load_npy_array(npy_path):\n",
    "    return np.load(npy_path, allow_pickle=True) #functionally equivalent to np.load(npy_array) but iterable; w/ Pickle\n",
    "\n",
    "\n",
    "def load_npy_df(npy_path):\n",
    "    return pd.DataFrame(np.load(npy_path, allow_pickle=True)) #load suite2p outputs as pandas dataframe\n",
    "\n",
    "\n",
    "def load_npy_dict(npy_path):\n",
    "    return np.load(npy_path, allow_pickle=True)[()] #load .npy as dictionary\n",
    "\n",
    "\"\"\"\n",
    "The following 3 func. are used to translate_suite2p_outputs_to_csv;\n",
    "check_for_suite2p_output is defined below: primarily for if iscell is not included (it always is)\n",
    "\n",
    "Then, we append the folder location of suite2p outputs into the current path (found_output_paths = files in os.walk(path))\n",
    "found_output_paths.append(current_path)\n",
    "\"\"\"\n",
    "\n",
    "def check_for_suite2p_output(path, check_for_iscell=False):\n",
    "    for file, path_to_file in SUITE2P_STRUCTURE.items():\n",
    "        if file == \"iscell\" and not check_for_iscell:\n",
    "            continue\n",
    "        if not os.path.isfile(os.path.join(path, *path_to_file)):\n",
    "            return False\n",
    "    return True\n",
    "    #Strictly to check for iscell.npy; I originally forgot to include this when Marti Ritter wrote the code\n",
    "    # I am also too scared to change this before my Master's less this break the pipeline\n",
    "    # For the synapses it also is not relatively important since we assume all detected ROIs are real* synapses\n",
    "\n",
    "\n",
    "def get_all_suite2p_outputs_in_path(path, check_for_iscell=False):\n",
    "    found_output_paths = []\n",
    "    \n",
    "    for current_path, directories, files in os.walk(path):\n",
    "        if check_for_suite2p_output(current_path, check_for_iscell=check_for_iscell):\n",
    "            found_output_paths.append(current_path)\n",
    "    return found_output_paths\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_suite2p_output(path, use_iscell=False):\n",
    "    \"\"\"here we define our suite2p dictionary from the SUITE2P_STRUCTURE...see above\"\"\"\n",
    "    suite2p_dict = {\n",
    "        \"F\": load_npy_array(os.path.join(path, *SUITE2P_STRUCTURE[\"F\"])),\n",
    "        \"Fneu\": load_npy_array(os.path.join(path, *SUITE2P_STRUCTURE[\"Fneu\"])),\n",
    "        \"stat\": load_npy_df(os.path.join(path, *SUITE2P_STRUCTURE[\"stat\"]))[0].apply(pd.Series),\n",
    "    }\n",
    "\n",
    "    if use_iscell == False:\n",
    "        suite2p_dict[\"IsUsed\"] = [(suite2p_dict[\"stat\"][\"skew\"] >= 0) & \n",
    "                                                (suite2p_dict['stat']['footprint'] < 3.0) &\n",
    "                                                (suite2p_dict['stat']['footprint'] > 0.0)]\n",
    "\n",
    "        suite2p_dict[\"IsUsed\"] = pd.DataFrame(suite2p_dict[\"IsUsed\"]).iloc[:,0:].values.T\n",
    "        suite2p_dict[\"IsUsed\"] = np.squeeze(suite2p_dict[\"IsUsed\"])\n",
    "    else:\n",
    "        suite2p_dict[\"IsUsed\"] = load_npy_df(os.path.join(path, *SUITE2P_STRUCTURE[\"iscell\"]))[0].astype(bool)\n",
    "\n",
    "    return suite2p_dict\n",
    "\"\"\"\n",
    "Possible to append this function further for synapse exclusion\n",
    " for example, append the document based on \n",
    "suite2p_dict[\"stat\"] using values for [\"skew\"]/[\"npix\"]/[\"compactness\"]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def translate_suite2p_dict_to_df(suite2p_dict):\n",
    "    \"\"\"this is the principle function in which we will create our .csv file structure; and where we will actually use\n",
    "        our detector functions for spike detection and amplitude extraction\"\"\"\n",
    "    spike_amplitudes = [single_synapse_baseline_correction_and_peak_return(f_trace, fneu_trace, return_peaks = False) \n",
    "                       for (f_trace, fneu_trace) in zip(suite2p_dict[\"F\"], suite2p_dict[\"Fneu\"])]\n",
    "    \n",
    "    spikes_per_neuron = [single_synapse_baseline_correction_and_peak_return(f_trace, fneu_trace) \n",
    "                             for (f_trace, fneu_trace) in zip(suite2p_dict[\"F\"], suite2p_dict[\"Fneu\"])]\n",
    "#spikes_per_neuron from single_cell_peak_return OUTPUT = list of np.arrays        \n",
    "    df = pd.DataFrame({\"IsUsed\": suite2p_dict[\"IsUsed\"],\n",
    "                       \"Skew\": suite2p_dict[\"stat\"][\"skew\"],\n",
    "                       \"PeakTimes\": spikes_per_neuron,\n",
    "                       \"Amplitudes\": spike_amplitudes,\n",
    "                       \"Total Frames\": len(suite2p_dict[\"F\"].T)})\n",
    "                       \n",
    "    df.index.set_names(\"SynapseID\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def translate_suite2p_outputs_to_csv(input_path, output_path, overwrite=False, check_for_iscell=False):\n",
    "    \"\"\"This will create .csv files for each video loaded from out data fram function below.\n",
    "        The structure will consist of columns that list: \"Amplitudes\": spike_amplitudes})\n",
    "        \n",
    "        col1: ROI #, col2: IsUsed (from iscell.npy); boolean, col3: Skew (from stats.npy); could be replaced with any \n",
    "        stat >> compactness, col3: spike frames (relative to input frames), col4: amplitude of each spike detected measured \n",
    "        from the baseline (the median of each trace)\"\"\"\n",
    "    \n",
    "    suite2p_outputs = get_all_suite2p_outputs_in_path(input_path)\n",
    "    \n",
    "    for suite2p_output in suite2p_outputs:\n",
    "        output_directory = os.path.basename(suite2p_output)\n",
    "        translated_path = os.path.join(output_path, f\"{output_directory}.csv\")\n",
    "        if os.path.exists(translated_path) and not overwrite:\n",
    "            print(f\"CSV file {translated_path} already exists!\")\n",
    "            continue\n",
    "\n",
    "        suite2p_dict = load_suite2p_output(suite2p_output)\n",
    "        suite2p_df = translate_suite2p_dict_to_df(suite2p_dict)\n",
    "\n",
    "        suite2p_df.to_csv(translated_path)\n",
    "\n",
    "        \n",
    "#For establishing ImageJ/CellProfiler baseline\n",
    "\"\"\"\n",
    "def load_imageJ_xl_sheet(imageJ_xl_output_path, xl_sheetname, return_array=True):\n",
    "    imageJ_xl = pd.read_excel(imageJ_xl_output_path, engine='openpyxl',\n",
    "                       sheet_name = xl_sheetname, header = None)\n",
    "    processed_imageJ_xl = imageJ_xl.T\n",
    "        #transposed to match the output of suite2p / make more legible for humans\n",
    "    processed_imageJ_xl_header = processed_imageJ_xl.iloc[0]\n",
    "    processed_imageJ_xl = processed_imageJ_xl[1:]\n",
    "    processed_imageJ_xl.columns = processed_imageJ_xl_header\n",
    "    processed_imageJ_xl_numpy_array = pd.DataFrame.to_numpy(processed_imageJ_xl)\n",
    "    if return_array == True:\n",
    "        return processed_imageJ_xl_numpy_array\n",
    "    else:\n",
    "        return processed_imageJ_xl\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
