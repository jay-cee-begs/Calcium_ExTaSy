{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebd51b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting syn_plotting_utility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile syn_plotting_utility.py\n",
    "#I probably will rename these files so I can tell them apart when I have a lot of tabs open...\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "below is an example structure for a dictionary for all the experiment files \n",
    "suite2one might implement this code as the following in a processing_pipeline:\n",
    "    experiment_structure = {\n",
    "    \"control\": {\n",
    "        \"replicate_1\": [f\"control_A_well_0{j}Dur180sInt100msBin500ms_filtered.pkl\" for j in range(1, 5)],\n",
    "        \"replicate_2\": [f\"control_A_well_0{j}Dur180sInt100msBin500ms_filtered.pkl\" for j in range(5, 9)],\n",
    "    },\n",
    "    \"treatment\": {\n",
    "        \"replicate_1\": [f\"treatment_B_well_0{j}Dur180sInt100msBin500ms_filtered.pkl\" for j in range(1, 5)],\n",
    "        \"replicate_2\": [f\"treatment_B_well_0{j}Dur180sInt100msBin500ms_filtered.pkl\" for j in range(5, 9)],\n",
    "    }\n",
    "}\n",
    "experiment_structure = {\n",
    "\n",
    "\"APV_treatment\": [f\"DIV1{i}_cs{j}_r{k}_APV*.pkl\" for i in range(5,9), j in range(1,3), k in range (1,3)],\n",
    "\"PDBu_treatment\": [f\"DIV1{i}_cs{j}_r{k}_PDBu*.pkl\" for i in range(5,9), j in range(1,3), k in range (1,3)],\n",
    "\"CNQX_treatment\": [f\"DIV1{i}_cs{j}_r{k}_CNQX*.pkl\" for i in range(5,9), j in range(1,3), k in range (1,3)],\n",
    "\"baseline\": [f\"DIV1{i}_cs{j}_r{k}_baseline*.pkl\" for i in range(5,9), j in range(1,3), k in range (1,3)]\n",
    "\n",
    "maybe consider a dictionary of dictionaries?? although I am not sure the benefit of this immediately\n",
    "}\n",
    "\"\"\"\n",
    "_experiment_structure_example = {\n",
    "    \"control\": {\n",
    "        \"dataset1\": [\"file1\", \"file2\"]\n",
    "    },\n",
    "    \"APV\": {\n",
    "        \"dataset2\": [\"file3\", \"file4\"]\n",
    "    },\n",
    "    \"PDBu\": {\n",
    "        \"dataset3\": [\"file1\", \"file2\"]\n",
    "    },\n",
    "    \"CNQX\": {\n",
    "        \"dataset4\": [\"file3\", \"file4\"]\n",
    "    },\n",
    "}\n",
    "\n",
    "_available_tests = {\n",
    "    \"mann-whitney-u\": stats.mannwhitneyu,\n",
    "    \"wilcoxon\": stats.wilcoxon,\n",
    "    \"paired_t\": stats.ttest_rel,\n",
    "}\n",
    "#If you cannot tell this is where I have spent the least time so far in coding...\n",
    "\n",
    "def get_significance_text(series1, series2, test=\"mann-whitney-u\", bonferroni_correction=1, show_ns=False, \n",
    "                          cutoff_dict={\"*\":0.05, \"**\":0.01, \"***\":0.001}, return_string=\"{text}\\n~{pvalue:.4f}\"):\n",
    "    statistic, pvalue = _available_tests[test](series1, series2)\n",
    "    levels, cutoffs = np.vstack(list(cutoff_dict.items())).T\n",
    "    levels = np.insert(levels, 0, \"n.s.\" if show_ns else \"\")\n",
    "    text = levels[(pvalue < cutoffs.astype(float)).sum()]\n",
    "    return return_string.format(pvalue=pvalue, text=text)\n",
    "\n",
    "def add_significance_bar_to_axis(ax, series1, series2, center_x, line_width):\n",
    "    significance_text = get_significance_text(series1, series2, show_ns=True)\n",
    "    \n",
    "    original_limits = ax.get_ylim()\n",
    "    \n",
    "    ax.errorbar(center_x, original_limits[1], xerr=line_width/2, color=\"k\", capsize=4)\n",
    "    ax.text(center_x, original_limits[1], significance_text, ha=\"center\", va=\"bottom\")\n",
    "    \n",
    "    extended_limits = (original_limits[0], (original_limits[1] - original_limits[0]) * 1.2 + original_limits[0])\n",
    "    ax.set_ylim(extended_limits)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def aggregated_feature_plot(experiment_df, feature=\"SpikesFreq\", agg_function=\"median\", comparison_function=\"mean\",\n",
    "                            palette=\"Set1\", significance_check=[\"control\", \"treatment\"]):\n",
    "    \n",
    "    grouped_df = experiment_df.groupby([\"dataset\", \"group\", \"file_name\"]).agg(agg_function).reset_index(drop=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot()\n",
    "    color_palette = sns.color_palette(palette)\n",
    "\n",
    "    sns.swarmplot(x=\"group\", y=feature, hue=\"dataset\", data=grouped_df, ax=ax, size=10, alpha=0.6, palette=palette)\n",
    "\n",
    "    marker_width = 0.5\n",
    "    tick_positions = {ax.get_xticklabels()[index].get_text(): ax.get_xticks()[index] for index in\n",
    "                      range(len(ax.get_xticklabels()))}\n",
    "    \n",
    "    for group, group_df in grouped_df.groupby(\"group\"):\n",
    "        for dataset_index, (dataset, dataset_df) in enumerate(group_df.groupby(\"dataset\")):\n",
    "            feature_mean = dataset_df[feature].agg(comparison_function)\n",
    "            ax.plot([tick_positions[group] - marker_width/2, tick_positions[group] + marker_width/2],\n",
    "                    [feature_mean, feature_mean], \"-\", color=color_palette[dataset_index], lw=2)\n",
    "\n",
    "    if significance_check:\n",
    "        sub_checks = [significance_check] if not any(isinstance(element, list) for element in significance_check) else significance_check\n",
    "        for sub_check in sub_checks:\n",
    "            add_significance_bar_to_axis(ax, \n",
    "                                 grouped_df[grouped_df[\"group\"] == sub_check[0]][feature], \n",
    "                                 grouped_df[grouped_df[\"group\"] == sub_check[1]][feature],\n",
    "                                (tick_positions[sub_check[0]] + tick_positions[sub_check[1]]) / 2,\n",
    "                                 abs(tick_positions[sub_check[0]] - tick_positions[sub_check[1]]))\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, frameon=False)\n",
    "\n",
    "    ax.set_ylim([0, ax.get_ylim()[1]])\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    return fig\n",
    "\n",
    "def build_experiment_dfs(input_path, experiment_structure):\n",
    "    experiment_cell_stats, experiment_binned_stats = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    for group in experiment_structure.keys():\n",
    "        for dataset in experiment_structure[group].keys():\n",
    "            for file_name in experiment_structure[group][dataset]:\n",
    "                file_dict = pd.read_pickle(os.path.join(input_path, file_name))\n",
    "                cell_stats, binned_stats = file_dict[\"cell_stats\"], file_dict[\"binned_stats\"]\n",
    "                \n",
    "                for stats, experiment_stats in zip((cell_stats, binned_stats), \n",
    "                                                   (experiment_cell_stats, experiment_binned_stats)):\n",
    "                    stats[[\"group\", \"dataset\", \"file_name\"]] = group, dataset, file_name\n",
    "                experiment_cell_stats = pd.concat((experiment_cell_stats, cell_stats))\n",
    "                experiment_binned_stats = pd.concat((experiment_binned_stats, binned_stats))\n",
    "    return experiment_cell_stats, experiment_binned_stats\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
