{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca20bf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stripped_suite2p_utility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stripped_suite2p_utility.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from stripped_detector_utility import *\n",
    "    #this is where all the detector functions will be used; at least initially\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SUITE2P_STRUCTURE describes the sequence of directories to traverse to arrive at the files named in the key.\n",
    "For example: \"F\": [\"suite2p\", \"plane0\", \"F.npy\"] --> File describing F is found at ./suite2p/plane0/F.npy.\n",
    "All locations are relative to the suite2p output folder, which is output into the folder \n",
    "/ file location where the original \n",
    ".tiff image for analysis was located. suite2p output can also be identified by \n",
    "checking if it follows this structure.\n",
    "Both spks and iscell are not used because: spks are not calculated; no need for deconvolution\n",
    "iscell is not used because all ROIs are considered and filtered later based on individual stats\n",
    "\"\"\"\n",
    "SUITE2P_STRUCTURE = {\n",
    "    \"F\": [\"suite2p\", \"plane0\", \"F.npy\"],\n",
    "    \"Fneu\": [\"suite2p\", \"plane0\", \"Fneu.npy\"],\n",
    "    #'spks': [\"suite2p\", \"plane0\", \"spks.npy\"],\n",
    "    \"stat\": [\"suite2p\", \"plane0\", \"stat.npy\"],\n",
    "    \"iscell\": [\"suite2p\", \"plane0\", \"iscell.npy\"],\n",
    "}\n",
    "\"\"\" spks is not really necessary with our current set up since the spont. events are all pretty uniform, and OASIS \n",
    "in suite2p udnerestimates and are below \n",
    "    AP threshold (and therefore will not need to be deconvolved into action potentials themselves)\"\"\"\n",
    "\n",
    "def load_npy_array(npy_path):\n",
    "    \"\"\"method to load suite2p output files into python, but callable; otherwise you could \"\"\"\n",
    "    return np.load(npy_path, allow_pickle=True) #functionally equivalent to np.load(npy_array) but iterable; w/ Pickle\n",
    "\n",
    "def load_npy_df(npy_path):\n",
    "    return pd.DataFrame(np.load(npy_path, allow_pickle=True)) #load suite2p outputs as pandas dataframe\n",
    "\n",
    "\n",
    "def load_npy_dict(npy_path):\n",
    "    return np.load(npy_path, allow_pickle=True)[()] #load .npy as dictionary\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The following 3 func. are used to translate_suite2p_outputs_to_csv;\n",
    "check_for_suite2p_output is defined below: primarily for if iscell is not included (it always is)\n",
    "\n",
    "Then, we append the folder location of suite2p outputs into the current path \n",
    "(found_output_paths = files in os.walk(path))\n",
    "found_output_paths.append(current_path)\n",
    "\"\"\"\n",
    "\n",
    "def check_for_suite2p_output(path, check_for_iscell=False):\n",
    "    for file, path_to_file in SUITE2P_STRUCTURE.items():\n",
    "#         check for iscell was here because I did not include it originally to the writer of this code; not necessary\n",
    "        if file == \"iscell\" and not check_for_iscell:\n",
    "            continue\n",
    "        if not os.path.isfile(os.path.join(path, *path_to_file)):\n",
    "            return False\n",
    "    return True\n",
    "    #Strictly to check for iscell.npy; I originally forgot to include this when Marti Ritter wrote the code\n",
    "    # I am also too scared to change this before my Master's less this break the pipeline\n",
    "    # For the synapses it also is not relatively important since we assume all detected ROIs are real* synapses\n",
    "\n",
    "\n",
    "def get_all_suite2p_outputs_in_path(path, check_for_iscell=False):\n",
    "    \"\"\"loads all the suite2p outputs into a processing path\"\"\"\n",
    "    found_output_paths = []\n",
    "    \n",
    "    for current_path, directories, files in os.walk(path):\n",
    "        if check_for_suite2p_output(current_path, check_for_iscell=check_for_iscell):\n",
    "            found_output_paths.append(current_path)\n",
    "    return found_output_paths\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_suite2p_output(path, use_iscell=False):\n",
    "    \"\"\"here we define our suite2p dictionary from the SUITE2P_STRUCTURE...see above\n",
    "    we also redefine what qualifies as a puncta vs. random rois with the 'IsUsed' setting \"\"\"\n",
    "    suite2p_dict = {\n",
    "        \"F\": load_npy_array(os.path.join(path, *SUITE2P_STRUCTURE[\"F\"])),\n",
    "        \"Fneu\": load_npy_array(os.path.join(path, *SUITE2P_STRUCTURE[\"Fneu\"])),\n",
    "        \"stat\": load_npy_df(os.path.join(path, *SUITE2P_STRUCTURE[\"stat\"]))[0].apply(pd.Series),\n",
    "    }\n",
    "\n",
    "    if use_iscell == False:\n",
    "        suite2p_dict[\"IsUsed\"] = [(suite2p_dict[\"stat\"][\"skew\"] >= 2.0) & \n",
    "                                              (suite2p_dict['stat']['npix'] <=200) & \n",
    "                                              (suite2p_dict['stat']['compact'] <= 1.25) & \n",
    "                                              (suite2p_dict['stat']['footprint']< 3.0)]\n",
    "\n",
    "        suite2p_dict[\"IsUsed\"] = pd.DataFrame(suite2p_dict[\"IsUsed\"]).iloc[:,0:].values.T\n",
    "        suite2p_dict[\"IsUsed\"] = np.squeeze(suite2p_dict[\"IsUsed\"])\n",
    "    else:\n",
    "        suite2p_dict[\"IsUsed\"] = load_npy_df(os.path.join(path, *SUITE2P_STRUCTURE[\"iscell\"]))[0].astype(bool)\n",
    "\n",
    "    return suite2p_dict\n",
    "\n",
    "\n",
    "\n",
    "def translate_suite2p_dict_to_df(suite2p_dict):\n",
    "    \"\"\"this is the principle function in which we will create our .csv file structure; \n",
    "    and where we will actually use\n",
    "        our detector functions for spike detection and amplitude extraction\n",
    "        we then also implement our detector functions to output spike times and amplitudes\"\"\"\n",
    "    \n",
    "    peak_amplitudes = [single_synapse_baseline_correction_and_peak_return(f_trace, fneu_trace, return_peaks = False) \n",
    "                       for (f_trace, fneu_trace) in zip(suite2p_dict[\"F\"], suite2p_dict[\"Fneu\"])]\n",
    "    \n",
    "    peaks_per_synapse = [single_synapse_baseline_correction_and_peak_return(f_trace, fneu_trace) \n",
    "                             for (f_trace, fneu_trace) in zip(suite2p_dict[\"F\"], suite2p_dict[\"Fneu\"])]\n",
    "#peaks_per_neuron from single_cell_peak_return OUTPUT = list of np.arrays        \n",
    "    df = pd.DataFrame({\"IsUsed\": suite2p_dict[\"IsUsed\"],\n",
    "                       \"Skew\": suite2p_dict[\"stat\"][\"skew\"],\n",
    "                       \"PeakTimes\": peaks_per_synapse,\n",
    "                       \"Amplitudes\": peak_amplitudes,\n",
    "                       \"Total Frames\": len(suite2p_dict[\"F\"].T)})\n",
    "                       \n",
    "    df.index.set_names(\"SynapseId\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def translate_suite2p_outputs_to_csv(input_path, output_path, overwrite=False, check_for_iscell=False):\n",
    "    \"\"\"This will create .csv files for each video loaded from out data fram function below.\n",
    "        The structure will consist of columns that list: Amplitudes PeakTimes and values from suite2p\n",
    "        These csvs are created from the dataframes that are generated above\n",
    "        In theory this is not needed, but a good intermediate to check how suite2p is \n",
    "        working and if the pipeline is doing\n",
    "        what you want it to do\n",
    "        col1: ROI #, col2: IsUsed (from iscell.npy); boolean, col3: Skew (from stats.npy); \n",
    "        could be replaced with any \n",
    "        stat >> compactness, col3: spike frames (relative to input frames), col4:\n",
    "        amplitude of each spike detected measured \n",
    "        from the baseline (the median of each trace)\"\"\"\n",
    "    \n",
    "    suite2p_outputs = get_all_suite2p_outputs_in_path(input_path)\n",
    "    \n",
    "    for suite2p_output in suite2p_outputs:\n",
    "        output_directory = os.path.basename(suite2p_output)\n",
    "        translated_path = os.path.join(output_path, f\"{output_directory}.csv\")\n",
    "        if os.path.exists(translated_path) and not overwrite:\n",
    "            print(f\"CSV file {translated_path} already exists!\")\n",
    "            continue\n",
    "\n",
    "        suite2p_dict = load_suite2p_output(suite2p_output)\n",
    "        suite2p_df = translate_suite2p_dict_to_df(suite2p_dict)\n",
    "\n",
    "        suite2p_df.to_csv(translated_path)\n",
    "\n",
    "        \n",
    "#For establishing ImageJ/CellProfiler baseline... also not directly in our pipeline\n",
    "# But this function is included as it is a way to load data into python\n",
    "\n",
    "def load_imageJ_xl_sheet(imageJ_xl_output_path, xl_sheetname, return_array=True):\n",
    "    \"\"\"Strictly was used to import the fluorescence output from ImageJ into python for calculating the random noise used\n",
    "    the detector utility functions\"\"\"\n",
    "    imageJ_xl = pd.read_excel(imageJ_xl_output_path, engine='openpyxl',\n",
    "                       sheet_name = xl_sheetname, header = None)\n",
    "    processed_imageJ_xl = imageJ_xl.T\n",
    "        #transposed to match the output of suite2p / make more legible for humans\n",
    "    processed_imageJ_xl_header = processed_imageJ_xl.iloc[0]\n",
    "    processed_imageJ_xl = processed_imageJ_xl[1:]\n",
    "    processed_imageJ_xl.columns = processed_imageJ_xl_header\n",
    "    processed_imageJ_xl_numpy_array = pd.DataFrame.to_numpy(processed_imageJ_xl)\n",
    "    if return_array == True:\n",
    "        return processed_imageJ_xl_numpy_array\n",
    "    else:\n",
    "        return processed_imageJ_xl\n",
    "    \n",
    "# From here we will go to the analysis utility / the detector utility \n",
    "# if you want to see how peaks and amplitudes are processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
